<!DOCTYPE html>
<html lang='en'><head>
  <title>Unsupervised Learning in R | Determination of Cluster Number - feo</title>
  <link rel='canonical' href='https://example.com/post/kmethods/' />
  <meta charset='utf-8'>
  <meta name='viewport' content='width=device-width, initial-scale=1' />
  <meta name='description' content='Comparison of the methods to determine optimal number of clusters' />
  <meta name='theme-color' content='#FD3519' />
  

  <meta name="generator" content="Hugo 0.111.3">

  





<link rel="stylesheet" href="https://example.com/sass/style.min.eabe1aa4bd266a15f7b39b122bd6a5cc75cb067e5373631ac21d7815d6240d6f.css" integrity="sha256-6r4apL0mahX3s5sSK9alzHXLBn5Tc2Mawh14FdYkDW8=" media="screen">
<link rel="stylesheet" href="https://example.com/syntax.min.css" integrity="" media="screen">

  <meta property="og:title" content="Unsupervised Learning in R | Determination of Cluster Number" />
<meta property="og:description" content="Comparison of the methods to determine optimal number of clusters" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://example.com/post/kmethods/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2023-03-02T19:49:05+02:00" />
<meta property="article:modified_time" content="2023-03-02T19:49:05+02:00" />

  <meta itemprop="name" content="Unsupervised Learning in R | Determination of Cluster Number">
<meta itemprop="description" content="Comparison of the methods to determine optimal number of clusters"><meta itemprop="datePublished" content="2023-03-02T19:49:05+02:00" />
<meta itemprop="dateModified" content="2023-03-02T19:49:05+02:00" />
<meta itemprop="wordCount" content="2449">
<meta itemprop="keywords" content="" />
</head>
<body>

  <header style="background-image:linear-gradient(
      rgba(0,0,0,0.4),rgba(0,0,0,0.4)
    ),url(&#39;https://example.com/images/background.jpg&#39;)">
  <div class="intro">
    <div class="logo-container">
      <a href="/">
        <img src='https://example.com/images/feopp.jpeg' alt="Profile Technical Training" class="rounded-logo">
      </a>
    </div>
    <h2>Fatih Emre Ozturk</h2>
    <h3>Data Scientist</h3>
    <div class="menu">
      

        <p>
            <a href="/about/">
                About
            </a>
        </p>

        <p>
            <a href="/portfolio/">
                Technical Training
            </a>
        </p>

        <p>
            <a href="/post/">
                Post
            </a>
        </p>

      
        
        <p>
            <a href="mailto:ozturkfemre@gmail.com" target="_blank" rel="external">
                email me
            </a>
        </p>
      
    </div>
  </div>

  <div class="socials">
      
  
    <a href="https://github.com/ozturkfemre" class="social-link" target="_blank" rel="noopener" ><div class="icon">
  <svg width="35px" height="35px" viewBox="0 0 115 115" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img">
  
  <path d="M102.679 0H12.32C5.52 0 0 5.519 0 12.321v90.358C0 109.48 5.519 115 12.321 115h90.358c6.802 0 12.321-5.519 12.321-12.321V12.32C115 5.52 109.481 0 102.679 0zM71.182 98.494c-2.156.385-2.952-.95-2.952-2.053 0-1.386.051-8.471.051-14.195 0-4.005-1.335-6.546-2.9-7.881C74.878 73.313 84.89 72.003 84.89 55.6c0-4.671-1.669-7.007-4.39-10.01.436-1.105 1.9-5.648-.436-11.552-3.568-1.104-11.731 4.595-11.731 4.595-3.389-.95-7.06-1.438-10.679-1.438-3.62 0-7.29.488-10.679 1.438 0 0-8.163-5.699-11.73-4.595-2.337 5.878-.899 10.422-.437 11.551-2.72 3.004-4.004 5.34-4.004 10.011 0 16.326 9.574 17.712 19.072 18.765-1.232 1.104-2.336 3.003-2.72 5.724-2.44 1.104-8.677 3.004-12.4-3.568-2.335-4.056-6.545-4.39-6.545-4.39-4.159-.05-.282 2.619-.282 2.619 2.772 1.283 4.723 6.212 4.723 6.212 2.49 7.624 14.4 5.057 14.4 5.057 0 3.568.052 9.37.052 10.422 0 1.104-.77 2.438-2.952 2.053C27.21 92.821 15.35 76.701 15.35 57.86c0-23.564 18.02-41.456 41.585-41.456s42.663 17.892 42.663 41.456c.026 18.842-11.474 34.988-28.416 40.635zM46 82.81c-.488.103-.95-.102-1.001-.436-.051-.385.282-.719.77-.822.488-.05.95.154 1.001.488.077.334-.257.668-.77.77zm-2.439-.23c0 .333-.385.615-.898.615-.565.052-.95-.23-.95-.616 0-.333.385-.616.899-.616.487-.051.95.231.95.616zm-3.516-.283c-.103.334-.616.488-1.053.334-.488-.103-.821-.488-.719-.822.103-.334.617-.488 1.053-.385.513.154.847.54.719.873zm-3.158-1.386c-.23.282-.718.23-1.104-.154-.385-.334-.487-.822-.23-1.053.23-.282.718-.23 1.103.154.334.334.462.847.231 1.053zm-2.336-2.336c-.23.154-.667 0-.95-.385-.282-.385-.282-.822 0-1.001.283-.231.72-.052.95.333.283.385.283.847 0 1.053zm-1.668-2.49c-.231.23-.616.103-.899-.154-.282-.334-.333-.719-.102-.899.23-.23.616-.102.898.154.282.334.334.72.103.899zm-1.72-1.9c-.103.231-.436.283-.719.103-.334-.154-.488-.436-.385-.667.103-.154.385-.231.719-.103.334.18.488.462.385.667z"/>
  
  </svg>
</div>
</a>
  

  
    <a href="https://www.linkedin.com/in/ozturkfemre" class="social-link" target="_blank" rel="noopener" ><div class="icon">
  <svg width="35px" height="35px" viewBox="0 0 115 115" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img">
  
  <path d="M106.786 0H8.189C3.67 0 0 3.722 0 8.291v98.418C0 111.278 3.67 115 8.189 115h98.597c4.518 0 8.214-3.722 8.214-8.291V8.29C115 3.722 111.304 0 106.786 0zm-72.03 98.571H17.713V43.69h17.07V98.57h-.025zm-8.522-62.377c-5.467 0-9.882-4.44-9.882-9.883 0-5.442 4.415-9.882 9.882-9.882 5.442 0 9.883 4.44 9.883 9.882a9.87 9.87 0 0 1-9.883 9.883zm72.414 62.377H81.604V71.875c0-6.366-.129-14.555-8.856-14.555-8.882 0-10.242 6.931-10.242 14.093V98.57H45.46V43.69h16.352v7.495h.23c2.285-4.312 7.855-8.856 16.147-8.856 17.25 0 20.458 11.372 20.458 26.158V98.57z"/>
  
  </svg>
</div>
</a>
  

  
    <a href="https://medium.com/@ozturkfemre" class="social-link" target="_blank" rel="noopener" ><div class="icon">
  <svg width="35px" height="35px" viewBox="0 0 115 115" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img">
  
  <path d="M0 0v115h115V0H0zm95.542 27.235l-6.16 5.905a1.81 1.81 0 0 0-.693 1.72v43.458c-.103.667.154 1.335.693 1.72l6.032 5.904v1.31h-30.29v-1.259l6.238-6.058c.616-.616.616-.795.616-1.72V43.074L54.625 87.123h-2.336l-20.202-44.05v29.52c-.18 1.233.257 2.49 1.13 3.39l8.111 9.83v1.31H18.277v-1.31l8.111-9.83a3.93 3.93 0 0 0 1.053-3.39v-34.14a2.93 2.93 0 0 0-.976-2.516l-7.213-8.702v-1.309h22.41l17.301 37.991 15.222-37.965h21.357v1.283z"/>
  
  </svg>
</div>
</a>
  

  </div>
</header>

<div class="mobile-header">
  <p> feo </p>
  <div class="hamburger">
    <div class="bar"></div>
    <div class="bar"></div>
    <div class="bar"></div>
  </div>
</div>

<div class="overlay-menu">
  <header>
    <div class="intro">
      <div class="logo-container">
        <a href="/">
          <img src='https://example.com/images/feopp.jpeg' alt="Profile Technical Training" class="rounded-logo">
        </a>
      </div>
      <h2>Fatih Emre Ozturk</h2>
      <h3>Data Scientist</h3>
      <div class="menu">
        

        <p>
            <a href="/about/">
                About
            </a>
        </p>

        <p>
            <a href="/portfolio/">
                Technical Training
            </a>
        </p>

        <p>
            <a href="/post/">
                Post
            </a>
        </p>

        
          
          <p>
              <a href="mailto:ozturkfemre@gmail.com" target="_blank" rel="external">
                  email me
              </a>
          </p>
        
      </div>
    </div>

    <div class="socials">
        
  
    <a href="https://github.com/ozturkfemre" class="social-link" target="_blank" rel="noopener" ><div class="icon">
  <svg width="35px" height="35px" viewBox="0 0 115 115" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img">
  
  <path d="M102.679 0H12.32C5.52 0 0 5.519 0 12.321v90.358C0 109.48 5.519 115 12.321 115h90.358c6.802 0 12.321-5.519 12.321-12.321V12.32C115 5.52 109.481 0 102.679 0zM71.182 98.494c-2.156.385-2.952-.95-2.952-2.053 0-1.386.051-8.471.051-14.195 0-4.005-1.335-6.546-2.9-7.881C74.878 73.313 84.89 72.003 84.89 55.6c0-4.671-1.669-7.007-4.39-10.01.436-1.105 1.9-5.648-.436-11.552-3.568-1.104-11.731 4.595-11.731 4.595-3.389-.95-7.06-1.438-10.679-1.438-3.62 0-7.29.488-10.679 1.438 0 0-8.163-5.699-11.73-4.595-2.337 5.878-.899 10.422-.437 11.551-2.72 3.004-4.004 5.34-4.004 10.011 0 16.326 9.574 17.712 19.072 18.765-1.232 1.104-2.336 3.003-2.72 5.724-2.44 1.104-8.677 3.004-12.4-3.568-2.335-4.056-6.545-4.39-6.545-4.39-4.159-.05-.282 2.619-.282 2.619 2.772 1.283 4.723 6.212 4.723 6.212 2.49 7.624 14.4 5.057 14.4 5.057 0 3.568.052 9.37.052 10.422 0 1.104-.77 2.438-2.952 2.053C27.21 92.821 15.35 76.701 15.35 57.86c0-23.564 18.02-41.456 41.585-41.456s42.663 17.892 42.663 41.456c.026 18.842-11.474 34.988-28.416 40.635zM46 82.81c-.488.103-.95-.102-1.001-.436-.051-.385.282-.719.77-.822.488-.05.95.154 1.001.488.077.334-.257.668-.77.77zm-2.439-.23c0 .333-.385.615-.898.615-.565.052-.95-.23-.95-.616 0-.333.385-.616.899-.616.487-.051.95.231.95.616zm-3.516-.283c-.103.334-.616.488-1.053.334-.488-.103-.821-.488-.719-.822.103-.334.617-.488 1.053-.385.513.154.847.54.719.873zm-3.158-1.386c-.23.282-.718.23-1.104-.154-.385-.334-.487-.822-.23-1.053.23-.282.718-.23 1.103.154.334.334.462.847.231 1.053zm-2.336-2.336c-.23.154-.667 0-.95-.385-.282-.385-.282-.822 0-1.001.283-.231.72-.052.95.333.283.385.283.847 0 1.053zm-1.668-2.49c-.231.23-.616.103-.899-.154-.282-.334-.333-.719-.102-.899.23-.23.616-.102.898.154.282.334.334.72.103.899zm-1.72-1.9c-.103.231-.436.283-.719.103-.334-.154-.488-.436-.385-.667.103-.154.385-.231.719-.103.334.18.488.462.385.667z"/>
  
  </svg>
</div>
</a>
  

  
    <a href="https://www.linkedin.com/in/ozturkfemre" class="social-link" target="_blank" rel="noopener" ><div class="icon">
  <svg width="35px" height="35px" viewBox="0 0 115 115" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img">
  
  <path d="M106.786 0H8.189C3.67 0 0 3.722 0 8.291v98.418C0 111.278 3.67 115 8.189 115h98.597c4.518 0 8.214-3.722 8.214-8.291V8.29C115 3.722 111.304 0 106.786 0zm-72.03 98.571H17.713V43.69h17.07V98.57h-.025zm-8.522-62.377c-5.467 0-9.882-4.44-9.882-9.883 0-5.442 4.415-9.882 9.882-9.882 5.442 0 9.883 4.44 9.883 9.882a9.87 9.87 0 0 1-9.883 9.883zm72.414 62.377H81.604V71.875c0-6.366-.129-14.555-8.856-14.555-8.882 0-10.242 6.931-10.242 14.093V98.57H45.46V43.69h16.352v7.495h.23c2.285-4.312 7.855-8.856 16.147-8.856 17.25 0 20.458 11.372 20.458 26.158V98.57z"/>
  
  </svg>
</div>
</a>
  

  
    <a href="https://medium.com/@ozturkfemre" class="social-link" target="_blank" rel="noopener" ><div class="icon">
  <svg width="35px" height="35px" viewBox="0 0 115 115" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img">
  
  <path d="M0 0v115h115V0H0zm95.542 27.235l-6.16 5.905a1.81 1.81 0 0 0-.693 1.72v43.458c-.103.667.154 1.335.693 1.72l6.032 5.904v1.31h-30.29v-1.259l6.238-6.058c.616-.616.616-.795.616-1.72V43.074L54.625 87.123h-2.336l-20.202-44.05v29.52c-.18 1.233.257 2.49 1.13 3.39l8.111 9.83v1.31H18.277v-1.31l8.111-9.83a3.93 3.93 0 0 0 1.053-3.39v-34.14a2.93 2.93 0 0 0-.976-2.516l-7.213-8.702v-1.309h22.41l17.301 37.991 15.222-37.965h21.357v1.283z"/>
  
  </svg>
</div>
</a>
  

    </div>
  </header>
</div>

  <div class="content-wrapper">
    
      <div class="breadcrumb">
  





<span >
  <a href="https://example.com/"></a>
   / 
</span>


<span >
  <a href="https://example.com/post/">POST</a>
   / 
</span>


<span  class="active">
  <a href="https://example.com/post/kmethods/">Unsupervised Learning in R | Determination of Cluster Number</a>
  
</span>

</div>

    
    <main id="content" class="post">

<h1>Unsupervised Learning in R | Determination of Cluster Number</h1>
<div class="reading-time">
  <div class="icon">
  <svg width="18px" height="18px" viewBox="0 0 115 115" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img">
  
  <path d="M57.5 11C29.05 11 6 34.05 6 62.5S29.05 114 57.5 114 109 90.95 109 62.5 85.95 11 57.5 11zm0 93.032c-22.947 0-41.532-18.585-41.532-41.532 0-22.947 18.585-41.532 41.532-41.532 22.947 0 41.532 18.585 41.532 41.532 0 22.947-18.585 41.532-41.532 41.532zm12.833-21.68L52.703 69.54a2.508 2.508 0 0 1-1.018-2.015V33.427a2.5 2.5 0 0 1 2.492-2.492h6.646a2.5 2.5 0 0 1 2.492 2.492v29.426l13.871 10.092c1.122.81 1.35 2.368.54 3.49l-3.904 5.377a2.51 2.51 0 0 1-3.489.54z"/>
  
  </svg>
</div>

  <span>12 minutes</span>
</div>

<div class="published-date">
  <div class="icon">
  <svg width="18px" height="18px" viewBox="0 0 115 115" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img">
  
  <path d="M77.577 51.23a1.807 1.807 0 0 0-2.2.342l-27.562 27.79a1.807 1.807 0 0 1-2.2.342l-14.008-9.702a1.807 1.807 0 0 0-2.2.342l-1.952 1.968c-.287.22-.456.568-.455.936.001.37.172.716.46.934L45.637 86.77a1.807 1.807 0 0 0 2.2-.342l31.709-31.97c.287-.22.456-.567.455-.936a1.175 1.175 0 0 0-.46-.933l-1.963-1.36z"/><path d="M97.304 20H80.512c-.041.34-.063.683-.064 1.026a5.986 5.986 0 0 0 1.256 4.1c.054.003.103.02.157.025a4.881 4.881 0 0 1 1.865-.025c3.05.562 4.984 3.907 4.32 7.47-.666 3.563-3.678 5.996-6.728 5.433a4.932 4.932 0 0 1-2.437-1.258c-6.018-1.378-10.445-7.795-10.445-15.745 0-.347.023-.685.04-1.026H34.579c-.041.34-.063.683-.064 1.026a5.986 5.986 0 0 0 1.256 4.1c.054.003.103.02.157.025a4.881 4.881 0 0 1 1.865-.025c3.05.562 4.984 3.907 4.32 7.47-.666 3.563-3.678 5.996-6.728 5.433a4.932 4.932 0 0 1-2.437-1.258c-6.018-1.378-10.445-7.795-10.445-15.745 0-.22.019-.434.025-.652a9.788 9.788 0 0 0-5.697 4.471 9.683 9.683 0 0 0-2.65 4.764L1.158 92.871c-.965 4.689 2.6 8.503 7.948 8.503h6.334v2.673c-.077 5.41 4.263 9.861 9.705 9.953h72.16c5.438-.095 9.774-4.546 9.694-9.953V29.953c.08-5.407-4.256-9.858-9.695-9.953zM10.078 96.653c-2.378 0-3.964-1.697-3.535-3.782L16.637 43.84h80.787L87.331 92.871a5.254 5.254 0 0 1-5.091 3.782H10.078zm91.535 7.394c.036 2.403-1.891 4.382-4.308 4.424h-72.16c-2.42-.04-4.352-2.018-4.32-4.424v-2.673h60.443c5.348 0 10.484-3.814 11.449-8.503l8.897-43.215v54.391z"/><path d="M34.814 33c1.243 0 2.251-1.057 2.251-2.36 0-1.305-1.008-2.362-2.25-2.362-2.04 0-4.313-3.194-4.313-7.778s2.272-7.778 4.312-7.778c1.227 0 2.536 1.163 3.386 3.084H43C41.716 11.19 38.578 8 34.814 8 29.871 8 26 13.49 26 20.5c0 7.009 3.871 12.5 8.814 12.5z"/>
  
  </svg>
</div>

  <span>March 2, 2023</span>
</div>

<hr>
<p>I plan to write many series of articles in my medium journey that I started with <a href="https://medium.com/@ozturkfemre/data-visualization-with-base-r-a3d6d4e2acdc">data visualization</a>. In addition to series such as Unsupervised Statistical Learning, Supervised Statistical Learning, Math for Data Science, Statistics and Probability, I will also touch on the intersection of my undergraduate degree in philosophy and data science. In this way, I think I can support analytical and critical thinking for the individual who is on a data science journey. I will also have some articles that I plan to combine culture and data science.</p>
<p>In this regard, I am with you with the first post of the first series, Unsupervised Statistical Learning. In this post, I will talk about the methods to determine the optimal number of clusters. In this post, as in every other post, I will talk about what methods mean, what they are used for and how to do it step by step. I will run all methods in R by using the k-means clustering algorithm and always use the same dataset (<a href="https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+%28diagnostic%29">Breast Cancer Wisconsin</a>).</p>
<hr>
<h3 id="why-do-we-need-to-determine-clusternumber">Why do we need to determine cluster number?</h3>
<p>The first reason is that the number of clusters must be predetermined for the clustering algorithms to work. For example, many algorithms such as k-means, k-medoids, hierarchical clustering need to know the number of clusters in order to work. Depending on the data set and the work done with that data set, we may know the number of clusters in advance. For example, in the plot below, it is quite possible to determine the number of clusters with a scatter plot.</p>
<p><img src="https://cdn-images-1.medium.com/max/800/1*b218oEdOwrx8g24HC13B5w.png" alt=""></p>
<hr>
<p>However, determining the number of clusters in commonly encountered data sets is too complex to be achieved simply by observing the overall structure of the data set with a scatter plot. For example, when the scatter plot below is analyzed, it is not possible to tell how many clusters the data set is divided into.</p>
<p><img src="https://cdn-images-1.medium.com/max/800/1*ekHDzQi6HseK6uGd6yv0Zg.png" alt=""></p>
<p>However, since situations like the one in the above plot are frequently encountered, some methods have been proposed to determine the number of clusters. I will now explain what some of these methods are, how they are calculated step by step and their implementation in R.</p>
<hr>
<h3 id="elbow-method">Elbow Method</h3>
<p>The elbow method, also known as <em>total within sum of squares</em>, is a technique used to determine the optimal number of clusters for a k-means clustering analysis. The idea behind the elbow method is to run k-means clustering on the dataset for a range of values of k (number of clusters), and for each value of k calculate the sum of squared distances of each point from its closest centroid (SSE). The elbow point is the point on the plot of SSE against the number of clusters (k) where the change in SSE begins to level off, indicating that adding more clusters doesn't improve the model much. [1], [2]</p>
<p>The steps to perform the elbow method are:</p>
<ol>
<li>
<p>Select a range of k values, usually from 1 to 10 or the square root of the number of observations in the dataset.</p>
</li>
<li>
<p>Run k-means clustering for each k value and calculate the SSE (sum of squared distances of each point from its closest centroid).</p>
</li>
<li>
<p>Plot the SSE for each k value.</p>
</li>
<li>
<p>The point on the plot where the SSE starts to decrease at a slower rate is the elbow point, and the corresponding number of clusters is the optimal value for k.</p>
</li>
</ol>
<p>Undoubtedly, the Elbow method is one of the most widely used methods. It can be said to be a method that follows the same logic as k-means. However, it would not be correct to say that it gives good results under all circumstances. Sometimes it can even be said to be misleading. For this reason, although I use the elbow method in every cluster analysis, I do not rely on it alone.</p>
<p>In R, <code>factoextra</code> packages offers fancy plots for some of the methods to determine optimal number of clusters in this post. I will use <code>fviz_nbclust</code> function to visualize elbow method for the dataset.</p>
<pre><code>fviz_nbclust(df, # data  
             kmeans, # clustering algorithm 
             nstart = 25, # if centers is a number, how many random sets should be chosen?(default is 25)
             iter.max = 200, # the maximum number of iterations allowed.
             method = &quot;wss&quot;) # elbow method
</code></pre>
<p><img src="https://cdn-images-1.medium.com/max/800/1*v5FiIzpDhJAHEE7DmosoOA.png" alt=""></p>
<p>For example, the output above shows that there is no sharp elbow. For this reason, it draws attention as a result open to interpretation. An interpretation based on this elbow may therefore lead to incorrect results.</p>
<hr>
<h3 id="average-silhouette-method">Average Silhouette Method</h3>
<p>Average silhouette method measures how well-defined a particular cluster is, and how well-separated it is from other clusters. At this point, it is necessary to state that Silhouette value is calculated for each observation in the data set. Average of the silhouette value of all observations gives us the average silhouette value, which is the silhouette value of the clustering analysis [3] , [4].</p>
<p>The steps to calculate silhouette value for a observation are:</p>
<ol>
<li>calculate cluster tightness: the average distance purple observation to all blue observations which all are in the same cluster, which is called a(i).</li>
</ol>
<p><img src="https://cdn-images-1.medium.com/max/800/1*oPJ8x71lbkMomHRx4qm2Ug.png" alt="illustration of calculating cluster tightness"></p>
<p>2. calculate cluster separation: observation's minimum distance to all the observations in a different cluster(yellow cluster), which is called b(i).</p>
<p><img src="https://cdn-images-1.medium.com/max/800/1*h0Ues0gLRs2FD5TrOpmFGA.png" alt="illustration of calculating cluster separation"></p>
<p>3. calculate silhouette coefficient: calculate its silhouette value &quot;s(i)&quot; as the difference between the b(i) and a(i), divided by the maximum of these two distances:</p>
<p><img src="https://cdn-images-1.medium.com/max/800/1*L2nNHjwqpS7ZOdXZEvTC2g.png" alt="calculation of silhouette coeffiecient"></p>
<p>After calculating silhouette coefficient of each observation, we can finally, calculate the average silhouette value for all observations: mean(si). To determine the number of clusters, we usually cluster each number of clusters for a range of 2 to 10 clusters and obtain the average silhouette value for each number of clusters. The silhouette value ranges from -1 to 1, where a value of 1 indicates a strong similarity to the other observations in its own cluster, and a value of -1 indicates a strong similarity to observations in another cluster. In other words, the number of clusters with the highest silhouette value is the number of clusters we will determine.</p>
<p>Again <code>fviz_nbclust</code> function is a way to see average silhoutte plot to decide the optimal number of clusters:</p>
<pre><code>fviz_nbclust(df, # data
             kmeans, # clustering algorithm
             method = &quot;silhouette&quot;) # silhouette
</code></pre>
<p><img src="https://cdn-images-1.medium.com/max/800/1*yxQGOAUuwKUAT66scEaWWg.png" alt=""></p>
<p>As can be easily seen from the plot, the clustering model with the highest silhouette value is the clustering for 2 clusters. Therefore, it can be inferred that the optimal number of clusters is two. However, when the silhouette values on the y-axis are examined, the silhouette value for the number of clusters 3 is also quite close, although the number of clusters 2 is the highest. For this reason, it would be more useful to always run the clustering algorithm for both 2 and 3 clusters and interpret the results.</p>
<hr>
<h3 id="gap-statistic-method">Gap Statistic Method</h3>
<p>Gap Statistic Method compares the observed within-cluster variation for different values of k with the variation expected under a null reference distribution of the data. [5]</p>
<p>The steps to perform the gap statistic method are:</p>
<ol>
<li>
<p>Select a range of k values, usually from 1 to 10 or the square root of the number of observations in the dataset.</p>
</li>
<li>
<p>Run the clustering algorithm (such as k-means or hierarchical clustering) for each k value and calculate the within-cluster variation Wk.</p>
</li>
<li>
<p>Generate B reference datasets by randomly sampling the original data and calculate the within-cluster variation W*k for each dataset.</p>
</li>
<li>
<p>Calculate the gap statistic</p>
</li>
<li>
<p>Plot the gap statistic for each k value.</p>
</li>
<li>
<p>The k value that corresponds to the maximum gap statistic is the optimal number of clusters.</p>
</li>
</ol>
<p>Again <code>fviz_nbclust</code> function is a way to see gap statistic plot to decide the optimal number of clusters:</p>
<pre><code>fviz_nbclust(df, 
             kmeans ,
             nstart = 25, 
             method = &quot;gap_stat&quot;)
</code></pre>
<p><img src="https://cdn-images-1.medium.com/max/800/1*mqekOsPoqAI2qWdSylkhrA.png" alt=""></p>
<p>Just like Average Silhouette Method, Gap Statistic Method is also offers 2 as the optimal number of clusters.</p>
<hr>
<h3 id="calinski---harabaszmethod">Calinski &mdash; Harabasz Method</h3>
<p>The Calinski-Harabasz index (also known as the Variance Ratio Criterion) is a commonly used evaluation metric for comparing different clustering solutions in unsupervised learning. It is a ratio of the between-cluster variance and the within-cluster variance, and it is used to determine the number of clusters that should be used in a clustering solution[6].</p>
<p>The steps to perform the calinski-harabasz method are:</p>
<ol>
<li>calculate within cluster sum of squares (WCSS): the sum of the squared distances between each observation and its corresponding cluster center(barycenter).</li>
</ol>
<p><img src="https://cdn-images-1.medium.com/max/800/1*4WNGuodDuuGJE_rCGrQwLw.png" alt=""></p>
<p>2. calculate between cluster sum of squares (BCSS): calculate the sum of the squared distances between each cluster center and the overall mean of all the observation.</p>
<p><img src="https://cdn-images-1.medium.com/max/800/1*aeLLLdSZe-juNaQIm2irbQ.png" alt=""></p>
<p>3. (BCSS / WCSS) &mdash; (n-k) / (k-1)</p>
<p>where,</p>
<p>k: total cluster number,</p>
<p>n:total observation number</p>
<p>The Calinski-Harabasz index ranges from 0 to infinity, with a higher value indicating a better clustering.</p>
<p>For the Calinski &mdash; Harabasz method, there is no visualization function in R as in the methods I mentioned before. For this reason, I will write a function that calculates and visualizes the Calinski &mdash; Harabasz values for the clusters 2 to 10 using the <code>calinhara</code> function in the <code>fpc</code> package.</p>
<pre><code>library(fpc) # for calinhara function

fviz_ch &lt;- function(data) {
  ch &lt;- c()
  for (i in 2:10) {
    km &lt;- kmeans(data, i) # perform clustering
    ch[i] &lt;- calinhara(data, # data
                       km$cluster, # cluster assignments
                       cn=max(km$cluster) # total cluster number
                       )
  }
  ch &lt;-ch[2:10]
  k &lt;- 2:10
  plot(k, ch,xlab =  &quot;Cluster number k&quot;,
       ylab = &quot;Caliński - Harabasz Score&quot;,
       main = &quot;Caliński - Harabasz Plot&quot;, cex.main=1,
       col = &quot;dodgerblue1&quot;, cex = 0.9 ,
       lty=1 , type=&quot;o&quot; , lwd=1, pch=4,
       bty = &quot;l&quot;,
       las = 1, cex.axis = 0.8, tcl  = -0.2)
  abline(v=which(ch==max(ch)) + 1, lwd=1, col=&quot;red&quot;, lty=&quot;dashed&quot;)
}

fviz_ch(df)
</code></pre>
<p><img src="https://cdn-images-1.medium.com/max/800/1*rmXp5yUexMVNx6L_5Mg6DA.jpeg" alt=""></p>
<p>As the other methods, Calinski &mdash; Harabasz methods is also offered 2 clusters for the dataset.</p>
<hr>
<h3 id="davies---bouldinmethod">Davies &mdash; Bouldin Method</h3>
<p>The Davies-Bouldin index (DBI) is a measure of the similarity between the clusters in a clustering solution. The DBI is calculated as the average similarity between each cluster and its most similar cluster, where the similarity between two clusters is defined as the maximum distance between any observation in one cluster and its closest observation in the other cluster. [7]</p>
<p>The steps to performDBI are:</p>
<ol>
<li>calculate intra-cluster dispersion: the average distance of all the data points in each cluster to the cluster center.</li>
</ol>
<p><img src="https://cdn-images-1.medium.com/max/800/1*aeLLLdSZe-juNaQIm2irbQ.png" alt=""></p>
<ol>
<li>calculate separation criteria: the Euclidean distance between the cluster centers for each pair of clusters.</li>
</ol>
<p><img src="https://cdn-images-1.medium.com/max/800/1*HtFg6se9Yzdt8jE5zRnLmw.png" alt=""></p>
<ol>
<li>find the most similar cluster: for each pair of clusters, calculate the similarity d(i, j) between the two clusters, as defined in the previous answer. Sum up the maximum similarity between each cluster and its most similar cluster. Divide the sum by the number of clusters to obtain the Davies-Bouldin index.</li>
</ol>
<p>The Davies-Bouldin index ranges from 0 to infinity, with a lower value indicating a better clustering solution. A DBI of 0 indicates that there is no similarity between any two clusters, while a high DBI value indicates that there is a high level of similarity between some of the clusters.</p>
<p>Just like the Calinski-Harabasz method, there is no visualization function in R as for Davies-Bouldin method. For this reason, I will write a function that calculates and visualizes the Davies &mdash; Bouldin value for the clusters 2 to 10 using the <code>NbClust</code> function in the <code>NbClust</code> package.</p>
<pre><code>library(NbClust)

fviz_db &lt;- function(data) {
  k &lt;- c(2:10)
  nb &lt;- NbClust(data, min.nc = 2, max.nc = 10, index = &quot;db&quot;, method = &quot;kmeans&quot;)
  db &lt;- as.vector(nb$All.index)
  plot(k, db,xlab =  &quot;Cluster number k&quot;,
       ylab = &quot;Davies-Bouldin Score&quot;,
       main = &quot;Davies-Bouldin Plot&quot;, cex.main=1,
       col = &quot;dodgerblue1&quot;, cex = 0.9 ,
       lty=1 , type=&quot;o&quot; , lwd=1, pch=4,
       bty = &quot;l&quot;,
       las = 1, cex.axis = 0.8, tcl  = -0.2)
  abline(v=which(db==min(db)) + 1, lwd=1, col=&quot;red&quot;, lty=&quot;dashed&quot;)
}


fviz_db(df)
</code></pre>
<p><img src="https://cdn-images-1.medium.com/max/800/1*JyF0ay_whg6ohuhv1w2pDg.jpeg" alt=""></p>
<p>Unlike other methods, we see that Davies-Bouldin's suggestion for the number of clusters is 7. Although it gives different results in this data set, it can give more reliable results in other data sets. For this reason, it would be useful to include the Davies-Bouldin method in every clustering analysis.</p>
<hr>
<h3 id="dunn-index">Dunn Index</h3>
<p>The Dunn index is a measure of the compactness and separability of the clusters in a clustering solution. It is calculated as the ratio of the minimum separation to the maximum diameter. [8]</p>
<p>The steps to perform Dunn Index are:</p>
<ol>
<li>calculate minimum separation: the smallest distance between the observations from two different clusters.</li>
</ol>
<p><img src="https://cdn-images-1.medium.com/max/800/1*EHsIVbk6_82UuuxrKn602Q.png" alt=""></p>
<p>2. calculate maximum diameter: the maximum distance between the observations in the same cluster.</p>
<p><img src="https://cdn-images-1.medium.com/max/800/1*Fl9-NiLKg17SXiz71CMHDw.png" alt=""></p>
<p>3. divide minimum separation by maximum diameter</p>
<p>The Dunn index ranges from 0 to infinity, with a higher value indicating a better clustering solution. A value of 1 indicates that the clusters are perfectly separated and perfectly compact, while a low value indicates that the clusters are either not separated or not compact.</p>
<p>Just like Calinski-Harabasz and Davies- Bouldin methods, there is no visualization function in R as for Dunn Index. For this reason, I will write a function that calculates and visualizes the Dunn Index values for the clusters 2 to 10 using the <code>dunn</code> function in the <code>clValid</code> package.</p>
<pre><code>library(clValid)

fviz_dunn &lt;- function(data) {
  k &lt;- c(2:10)
  dunnin &lt;- c()
  for (i in 2:10) {
    dunnin[i] &lt;- dunn(distance = dist(data), clusters = kmeans(data, i)$cluster)
  }
  dunnin &lt;- dunnin[2:10]
  plot(k, dunnin, xlab =  &quot;Cluster number k&quot;,
       ylab = &quot;Dunn Index&quot;,
       main = &quot;Dunn Plot&quot;, cex.main=1,
       col = &quot;dodgerblue1&quot;, cex = 0.9 ,
       lty=1 , type=&quot;o&quot; , lwd=1, pch=4,
       bty = &quot;l&quot;,
       las = 1, cex.axis = 0.8, tcl  = -0.2)
  abline(v=which(dunnin==max(dunnin)) + 1, lwd=1, col=&quot;red&quot;, lty=&quot;dashed&quot;)
}

fviz_dunn(df)
</code></pre>
<p><img src="https://cdn-images-1.medium.com/max/800/1*nB0iTIEA1mnhuiCZHIyecw.jpeg" alt=""></p>
<p>As the Davies-Bouldin methods, Dunn also suggested different cluster number. As I said before, each method may give different results for each data set. For this reason, it is useful to compare all methods in each clustering analysis.</p>
<hr>
<h4 id="references">References</h4>
<p>[1] Steinley, D., &amp; Brusco, M. J. (2011). Choosing the number of clusters in Κ-means clustering. Psychological methods, 16(3), 285.</p>
<p>[2] Halkidi, Maria, Yannis Batistakis, and Michalis Vazirgiannis. &quot;On clustering validation techniques.&quot; Journal of intelligent information systems 17 (2001): 107&ndash;145.</p>
<p>[3] Rousseeuw, Peter J. Silhouettes: a graphical aid to the interpretation and validation of cluster analysis.Journal of computational and applied mathematics, 1987, 20: 53&ndash;65.</p>
<p>[4] Halkidi, M., Batistakis, Y., &amp; Vazirgiannis, M. (2001). On clustering validation techniques. Journal of intelligent information systems, 17, 107&ndash;145.</p>
<p>[5] Tibshirani, R., Walther, G., &amp; Hastie, T. (2001). Estimating the number of clusters in a data set via the gap statistic. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2), 411&ndash;423.</p>
<p>[6] Caliński, T., &amp; Harabasz, J. (1974). A dendrite method for cluster analysis. Communications in Statistics-theory and Methods, 3(1), 1&ndash;27.</p>
<p>[7] Davies, D. L., &amp; Bouldin, D. W. (1979). A cluster separation measure. IEEE transactions on pattern analysis and machine intelligence, (2), 224&ndash;227.</p>
<p>[8] Dunn, J. C. (1973). A fuzzy relative of the ISODATA process and its use in detecting compact well-separated clusters.</p>


    </main>
  </div>
  <footer>
    <div class="footer-wrapper">
      <p>Made with ❤️ &mdash; Powered by <a href="https://gohugo.io/" target="_blank" rel="external">Hugo</a> and the <a href="https://github.com/bjacquemet/personal-web" target='_blank' rel="external">Personal Web</a> theme. Icons come from the great <a href="https://fontawesome.com/license" target="_blank" rel="external">Font Awesome</a> library</p>
      <p>© Fatih Emre Ozturk</p>
    </div>
  </footer>
  <link href="https://fonts.googleapis.com/css?family=Montserrat:500,600|Raleway:400,400i,600" rel="stylesheet">
  
  <script type="text/javascript">
    document.querySelector('.mobile-header').addEventListener('click', function () {
      var om = document.querySelector(".overlay-menu");
      if (document.querySelector('.hamburger').classList.contains("cross")) {
        document.querySelector('.hamburger').classList.remove("cross");
        om.style.display = "none";
        om.style.width = "0%";
        om.style.height = "0%";
      }
      else {
        document.querySelector('.hamburger').classList.add("cross");
        om.style.width = "100%";
        om.style.height = "100vh";
        om.style.display = "block";
      }
    });
  </script>
</body>
</html>
